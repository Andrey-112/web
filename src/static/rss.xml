<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[DevTube the place where developer videos live]]></title>
        <description><![CDATA[DevTube the place where developer videos live]]></description>
        <link>https://dev.tube</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 08 Aug 2018 04:30:33 GMT</lastBuildDate>
        <atom:link href="https://dev.tube/rss.xml" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Holden Karau and Cheburashka interview at JOTB2018]]></title>
            <description><![CDATA[Interview with Holden Karau during J On The Beach 2018, Málaga, Spain.]]></description>
            <link>https://dev.tube/video/-WHc01dJlAw</link>
            <guid isPermaLink="true">https://dev.tube/video/-WHc01dJlAw</guid>
            <pubDate>Wed, 08 Aug 2018 01:34:16 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Playing well together: Big data beyond the JVM with Spark and friends - Strata SJ 2018]]></title>
            <description><![CDATA[Many of the recent big data systems, like Hadoop, Spark, and Kafka, are written primarily in JVM languages. At the same time, there is a wealth of tools for data science and data analytics that exist outside of the JVM. Holden Karau and Rachel Warren explore the state of the current big data ecosystem and explain how to best work with it in non-JVM languages. While much of the focus will be on Python + Spark, the talk will also include interesting anecdotes about how these lessons apply to other systems (including Kafka).

Holden and Rachel detail how to bridge the gap using PySpark and discuss other solutions like Kafka Streams as well. They also outline the challenges of pure Python solutions like dask. Holden and Rachel start with the current architecture of PySpark and its evolution. They then turn to the future, covering Arrow-accelerated interchange for Python functions, how to expose Python machine learning models into Spark, and how to use systems like Spark to accelerate training of traditional Python models. They also dive into what other similar systems are doing as well as what the options are for (almost) completely ignoring the JVM in the big data space.

Python users will learn how to more effectively use systems like Spark and understand how the design is changing. JVM developers will gain an understanding of how to Python code from data scientist and Python developers while avoiding the traditional trap of needing to rewrite everything.]]></description>
            <link>https://dev.tube/video/fNtyDpWOA2k</link>
            <guid isPermaLink="true">https://dev.tube/video/fNtyDpWOA2k</guid>
            <pubDate>Wed, 08 Aug 2018 01:34:16 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Beyond Shuffling: Scaling Apache Spark by Holden Karau]]></title>
            <description><![CDATA[This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Abstract:
This session will cover our & community experiences scaling Spark jobs to large datasets and the resulting best practices along with code snippets to illustrate.
 
The planned topics are:
* Using Spark counters for performance investigation
* Spark collects a large number of statistics about our code, but how often do we really look at them? We will cover how to investigate performance issues and figure out where to best spend our time using both counters and the UI.
* Working with Key/Value Data
* Replacing groupByKey for awesomeness
groupByKey makes it too easy to accidently collect individual records which are too large to process. We will talk about how to replace it in different common cases with more memory efficient operations.
* Effective caching & checkpointing
* Being able to reuse previously computed RDDs without recomputing can substantially reduce execution time. Choosing when to cache, checkpoint, or what storage level to use can have a huge performance impact.
* Considerations for noisy clusters
* Functional transformations with Spark Datasets
*How to have the some of benefits of Spark’s DataFrames while still having the ability to work with arbitrary Scala code]]></description>
            <link>https://dev.tube/video/0KGGa9qX9nw</link>
            <guid isPermaLink="true">https://dev.tube/video/0KGGa9qX9nw</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[OSCON 2016 - Getting started contributing to Apache Spark by Holden Karau (IBM)]]></title>
            <description><![CDATA[Apache Spark is one of the most popular tools for big data and with 400+ open pull requests as of this writing, very active in terms of development as well. With such a large volume of contributions, it can be hard to know how to begin contributing yourself. Holden Karau offers a developer-focused head start, walking you through how to find good issues, formatting code, finding reviewers, and what to expect in the code review process. Holden also explores alternatives to contributing to Apache Spark directly (such as creating packages).]]></description>
            <link>https://dev.tube/video/2SC8BkKILXc</link>
            <guid isPermaLink="true">https://dev.tube/video/2SC8BkKILXc</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Holden Karau, IBM - #BigDataNYC 2016 - #theCUBE]]></title>
            <description><![CDATA[01. Holen Karau, IBM, Visits #theCUBE!. (00:22)
02. What's New In Your World With Spark. (01:07)
03. What Is Junky API. (02:010)
04. How Does Spark  Address Complexity. (03:51)
05. Is The Open Source Community Finding A Way To Colaberate. (05:47)
06. Is The  Open Source  Community Working To  Find It's Way  Into The Tools. (07:34)
07. How Much Of A Data Scientist Do You Have To Be. (08:37)
08. What Is Your Perspective On Machine Learning. (10:01)
09. Is Sampling Dead. (12:17)
10. Are We Going To Start Turning Big Data On The Problem Of Big Data. (13:05)

Track List created with http://www.vinjavideo.com.
--- ---
Collaboration and machine learning with Spark | #BigDataNYC
by Brittany Greaner | Sep 29, 2016

Open-source technology is paving the way to the future of affordable and flexible IT, and the Apache Spark open-source processing engine is no exception. There is even a Spark components page where users can share useful tools and technology.

“Even vendors are sharing,” said Holden Karau, principal software engineer at IBM. This enables much wider collaboration throughout the community, as well as narrower collaboration between friends and colleagues. “If you have a notebook and share it with your friend, you can work together more collaboratively. A lot of companies are building notebook solutions,” added Karau.

Karau was interviewed by Dave Vellante (@dvellante) and Peter Burris (@plburris), hosts of theCUBE, from the SiliconANGLE Media team, during BigDataNYC 2016 in New York, NY.

Spark’s range of complexity

Another standout feature of Spark is its range of complexity. It allows users who may not have much knowledge of Python or Java to be able to build what they need without that coding ability.
At the same time, if users of Spark do understand coding, they can also use that knowledge to their benefit to create exactly what they want. “I think Spark does a good job of being user friendly. With Spark it’s much simpler and exposed in ways people are already used to working with their data,” said Karau.

Machine learning

Another area Spark and other platforms are beginning to dive into is machine learning. The traditional method is to down-sample, but this isn’t the most efficient or thorough method. Machine learning allows a much wider, agile way of doing things.

“When you move people to a laptop, you can train an algorithm to 
recommend datasets to people,” said Karau. “The combination of notebooks and Spark means data scientists can directly apply data during the exploration phase.” It speeds up the process by eliminating the need to consult coworkers for their data sets or do manual searches. And that is very powerful, with strong implications for the future, Karau concluded.

#BigDataNYC
#theCUBE]]></description>
            <link>https://dev.tube/video/6iH6yAa8wFM</link>
            <guid isPermaLink="true">https://dev.tube/video/6iH6yAa8wFM</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Philly ETE 2017 #40 - Scaling with Apache Spark (or a lesson in unintended consequences) - H. Karau]]></title>
            <description><![CDATA[Holden Karau: "Scaling with Apache Spark (or a lesson in unintended consequences)"

Apache Spark is one the most popular general purpose distributed systems in the past few years. Apache Spark has APIs in Scala, Java, Python and more recently a few different attempts to provide support for R, C#, and Julia. This talk looks at Apache Spark from a performance/scaling point of view and the work we need to do to be able to handle large datasets. In essence parts of this talk could be considered “the impact of design decisions from years ago and how to work around them.” It’s not all doom and gloom though, we will explore the new APIs and the exciting new things we can do with them with a brief detour into how to work around some of the trade-offs in the new APIs – but mostly focused on the new exciting shiny things we can play with. A basic background with Apache Spark will probably make the talk more exciting, or depressing depending on your point of view, but for those new to Apache Spark just enough to understand whats going will be covered at the start. The presenter would of course encourage you to buy and read her books on the topic (“Learning Spark” & “High Performance Spark”), because which presenter doesn’t do that.]]></description>
            <link>https://dev.tube/video/7S8zJPICSYg</link>
            <guid isPermaLink="true">https://dev.tube/video/7S8zJPICSYg</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Debugging PySpark: Spark Summit East talk by Holden Karau]]></title>
            <link>https://dev.tube/video/A0jYQlxc2FU</link>
            <guid isPermaLink="true">https://dev.tube/video/A0jYQlxc2FU</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Sparkling Pandas - using Apache Spark to scale Pandas - Holden Karau and Juliet Hougland]]></title>
            <description><![CDATA[Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. We will introduce Sparkling Pandas, a new library that brings together the best features of Pandas and PySpark; Expressiveness, speed, and scalability.]]></description>
            <link>https://dev.tube/video/AcyI_V8FeIU</link>
            <guid isPermaLink="true">https://dev.tube/video/AcyI_V8FeIU</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Distributed Data Show Episode 34: Spark 2.3 with Holden Karau]]></title>
            <description><![CDATA[Patrick McFadin catches up with Holden Karau of Google to learn about new features of Spark 2.3, including Vectorized UDFs, Microbatch improvements, and Kubernetes support. Along the way, they explore whether API stability is an indicator that it’s time to make a career move. 

Highlights!
0:15 - Welcoming Holden
0:35 - Patrick asks Holden why Spark APIs keep changing and whether API stability and boring infrastructure is a good thing
2:43 - Big changes in Spark 2.3 include Vectorized UDFs powered by Apache Arrow, which gives a big performance boost to transferring data between Python and the JVM (for those who aren’t fans of Scala)
6:44 - In the new Spark Microbatch API, sources and sinks are no longer tied to batches. This gives the flexibility to process as quickly as possible when you can tolerate some data loss like some IoT and machine learning use cases
11:38 - Kubernetes support is finally in Spark 2.3 after a few competing approaches were resolved, simplifying deployment of complex Spark apps that leverage non-JVM libraries
14:44 Holden explains why Spark struggles with scaling down and how Kubernetes support may be part of the solution.
18:34 - Patrick and Holden discuss when it will be safe to deploy Spark on K8S in production (hint, it should be before Spark 3)


ABOUT DATASTAX ENTERPRISE 5.1
DataStax Enterprise 5.1, the database platform for cloud applications, includes Apache Cassandra 3.x with materialized views, tiered storage and advanced replication. Introduced in 5.1 is DataStax Enterprise Graph, the first graph database fast enough to power customer-facing applications, scale to massive datasets and integrate advanced tools to power deep analytical queries.

Learn more at http://www.datastax.com/products/datastax-enterprise and https://academy.datastax.com/resources/whats-new-datastax-enterprise-50

CONNECT WITH DATASTAX
Subscribe: http://www.youtube.com/c/datastax?sub_confirmation=1 
Site: http://datastax.com 
Facebook: https://facebook.com/datastax 
Twitter: https://twitter.com/datastax 
Linkedin: https://www.linkedin.com/company/datastax
http://feeds.feedburner.com/datastax 
https://github.com/datastax 

ABOUT DATASTAX ACADEMY
On the DataStax Academy YouTube channel, you can find tutorials, webinars and much more to help you learn and stay updated with the latest information on DataStax Enterprise©.  Create an account on https://academy.datastax.com to watch our free online courses, tutorials, and more.]]></description>
            <link>https://dev.tube/video/Becr6YPF9dQ</link>
            <guid isPermaLink="true">https://dev.tube/video/Becr6YPF9dQ</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Dealing with Contributor Overload]]></title>
            <description><![CDATA[Holden Karau

http://lca2018.linux.org.au/schedule/presentation/36/

The first external person contributing to our project is amazing, but when that 1 snowballs to 1,000 life can get a little bit stressful. All of these fine lovely people want to help, but somehow no one seems to want to help you deal with all of the code reviews, proposed documentation changes, or keeping your testing infrastructure alive (or three people want to help in different directions).

This talk explores what happens as a community grows using the speakers experience in her own personal projects (which have much less than 1k contributors) as well as larger projects, like Apache Spark.

Come for the being told its not your fault, stay for the techniques to avoid pissing everyone off.

P.S.

If the speaker is behind on reviewing one of your pull requests she is very sorry and would like to offer you a sticker and hope this talk explains some of why she is late.

This talk was given at Linux.conf.au 2018 (LCA2018) which was held on 22-26 January 2018 in Sydney Australia.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

#linux.conf.au #linux #foss #opensource]]></description>
            <link>https://dev.tube/video/BempWfBkvs8</link>
            <guid isPermaLink="true">https://dev.tube/video/BempWfBkvs8</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Spark Structured Streaming for Machine Learning w/ Holden Karau & Seth Hendrickson (IBM) StrataNY]]></title>
            <description><![CDATA[Streaming machine learning is being integrated in future versions of Spark, but you don’t need to wait. Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Spark’s new Structured Streaming and walk you through creating your own streaming model. Holden and Seth will also cover how to use structured machine-learning algorithms (if they are merged by the talk). By the end of this session, you’ll have a better understanding of Spark’s Structured Streaming API as well as how machine learning works in Spark.

See the companion blog post at https://www.oreilly.com/learning/extend-structured-streaming-for-spark-ml & slides at http://www.slideshare.net/hkarau/apache-spark-structured-streaming-for-machine-learning-strataconf-2016 for more information.]]></description>
            <link>https://dev.tube/video/Bmfek0Td-f8</link>
            <guid isPermaLink="true">https://dev.tube/video/Bmfek0Td-f8</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Extending Spark Machine Learning   Beyond Linear Regression   by Holden Karau]]></title>
            <description><![CDATA[This talk was recorded at Scala Days Chicago, 2017. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Apache Spark is one of the most popular Big Data tools. This talk introduces & takes a deep dive on Spark's scikit-learn inspired Machine Learning pipelines.

Spark's ML pipelines provide a lot of power, but sometimes the tools you need for your specific problem aren't available yet. By integrating your own data preparation and machine learning tools into Spark's ML pipelines you will be able to take advantage of useful meta-algorithms, like parameter searching.

Even if you don't have your own machine learning algorithms you want to implement, this talk peels back the covers on how the ML APIs are built and can help you make even more awesome ML pipelines and customize Spark models for your needs.

A basic understanding of Spark will make it easier to follow along, but if this is your first Spark talk, this will still be useful and give you a broad understanding of how Spark ML functions (of course since the presenter is an author, if this is your first introduction to Spark she encourages you to buy her book "Learning Spark" & "High Performance Spark").]]></description>
            <link>https://dev.tube/video/CAqqd4UqCtY</link>
            <guid isPermaLink="true">https://dev.tube/video/CAqqd4UqCtY</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Extending Spark Machine Learning - Holden Karau]]></title>
            <description><![CDATA[At the sold-out Spark & Machine Learning Meetup in Brussels on October 27, 2016, Holden Karau of the Spark Technology Center delivered a lightning talk called A very brief introduction to extending Spark ML for custom models: Talk + Demo.

Holden took a look at Apache SparkML™ pipelines. Inspired by sci-kit learn, they have the potential to make machine learning tasks much easier. This talk looked at how to extend Spark ML with custom model types when the built-in options don't meet your needs.

For more information about the Spark Technology Center: http://www.spark.tc/

Follow us: @apachespark_tc
Location: San Francisco, CA

Apache®, Apache Spark™, and Spark™ are trademarks of the Apache Software Foundation in the United States and/or other countries.]]></description>
            <link>https://dev.tube/video/GXjrH3soIuE</link>
            <guid isPermaLink="true">https://dev.tube/video/GXjrH3soIuE</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Testing & validating Apache Spark jobs by Holden Karau]]></title>
            <description><![CDATA[This talk was recorded at BeeScala 2017 in Ljubljana, Slovenia. Follow along on Twitter @BeeScalaConf and on the website for more information http://bee-scala.org.

Abstract:

As Spark continues to evolve, we need to revisit our testing techniques to support Datasets, streaming, and more. This talk expands on “Beyond Parallelize and Collect” (not required to have been seen) to discuss how to create large scale test jobs while supporting Spark’s latest features. We will explore the difficulties with testing Streaming Programs, options for setting up integration testing, beyond just local mode, with Spark, and also examine best practices for acceptance tests.]]></description>
            <link>https://dev.tube/video/LP6blh4TFqI</link>
            <guid isPermaLink="true">https://dev.tube/video/LP6blh4TFqI</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Extending Spark ML for your custom algorithms - Holden Karau]]></title>
            <link>https://dev.tube/video/Q85aUoyWyfc</link>
            <guid isPermaLink="true">https://dev.tube/video/Q85aUoyWyfc</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Holden Karau, IBM   Big Data SV 17   #BigDataSV   #theCUBE]]></title>
            <description><![CDATA[Holden Karau, from IBM, talks with Jeff Frick & George Gilbert at Big Data SV 2017 at the historic Pagoda Lounge in San Jose, Ca.

#BigDataSV
#theCUBE]]></description>
            <link>https://dev.tube/video/TpemEZ92Rdg</link>
            <guid isPermaLink="true">https://dev.tube/video/TpemEZ92Rdg</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Distributed Data Show Episode 38: Spark 3.0 and Beyond with Holden Karau]]></title>
            <description><![CDATA[David Gilardi talks with Holden Karau of Google to mine many wonderful nuggets on the future of Spark and find out what might happen if she had a magic wand of awesomeness.

Highlights!

0:15 - Welcoming Holden back to the show
0:30 - So what exactly is going to be in Spark 3? Significant updates to the SQL and Machine Learning (ML) APIs. There are missing pieces in ML API, adding them will cause breaking changes to existing models. One example is support for online model serving.
2:25 - The DataSet API does not yet fully cover all needed cases, causing developers to jump back to RDD APIs, so some API changes will be needed there . There will be continued performance improvements in query planning in minor releases.
3:13 - Python changes could include changes to handle Vectorized UDFs in the RDD APIs
4:35 Why it’s so hard to pin down when Spark 3 will appear: breaking API changes have to be worth it. We need to wait until the payoff in capability is worth the breaking. An example would be making ML APIs typesafe.
6:57 - What Holden would change in Spark, given a magic wand - shared memory buffer between languages using Apache Arrow
9:46 - Wrapping up - the most exciting change likely to be in Spark 3 in online model serving

ABOUT DATASTAX ENTERPRISE 5.1
DataStax Enterprise 5.1, the database platform for cloud applications, includes Apache Cassandra 3.x with materialized views, tiered storage and advanced replication. Introduced in 5.1 is DataStax Enterprise Graph, the first graph database fast enough to power customer-facing applications, scale to massive datasets and integrate advanced tools to power deep analytical queries.

Learn more at http://www.datastax.com/products/datastax-enterprise and https://academy.datastax.com/resources/whats-new-datastax-enterprise-50

CONNECT WITH DATASTAX
Subscribe: http://www.youtube.com/c/datastax?sub_confirmation=1 
Site: http://datastax.com 
Facebook: https://facebook.com/datastax 
Twitter: https://twitter.com/datastax 
Linkedin: https://www.linkedin.com/company/datastax
http://feeds.feedburner.com/datastax 
https://github.com/datastax 

ABOUT DATASTAX ACADEMY
On the DataStax Academy YouTube channel, you can find tutorials, webinars and much more to help you learn and stay updated with the latest information on DataStax Enterprise©.  Create an account on https://academy.datastax.com to watch our free online courses, tutorials, and more.]]></description>
            <link>https://dev.tube/video/UhlyzFusC5I</link>
            <guid isPermaLink="true">https://dev.tube/video/UhlyzFusC5I</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Getting The Best Performance With PySpark]]></title>
            <link>https://dev.tube/video/V6DkTVvy9vk</link>
            <guid isPermaLink="true">https://dev.tube/video/V6DkTVvy9vk</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Holden Karau - Spark - Where the Magic Breaks]]></title>
            <link>https://dev.tube/video/ZoNT9tVK6_E</link>
            <guid isPermaLink="true">https://dev.tube/video/ZoNT9tVK6_E</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[sfspark.org: Holden Karau, PySpark Beyond Shuffling -- Why it isn't Magic]]></title>
            <description><![CDATA[PySpark Beyond Shuffling -- Why it isn't Magic -- but also where there is some really cool magic 
Holden Karau -- Apache Spark Committer, Spark Technology Center, IBM]]></description>
            <link>https://dev.tube/video/eR8p7cpvHNs</link>
            <guid isPermaLink="true">https://dev.tube/video/eR8p7cpvHNs</guid>
            <pubDate>Wed, 08 Aug 2018 01:33:23 GMT</pubDate>
        </item>
    </channel>
</rss>